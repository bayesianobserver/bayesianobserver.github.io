A diffusion model is similar to a denoising autoencoder, but not quite the same. In a denoising autoencoder, a noisy piece of data is passed as input and the output is expected to be the noise-free data. This is similar to what's going on in each step of a diffusion model. Of course in a diffusion model, there are multiple different levels of noise, while in a a denoising autoencoder there is just one level of noise. In this sense, a diffusion model is like a sequence of different denoising autoencoders, each trained on data with a different noise leve. Also, the very first step of a diffusion process at inference time is pure noise - we don't ever have pure noise as input in a denoising autoencoder (although it would be interesting to see what the output would be). The first step of a diffusion model at inference time (i.e. the first denoising step) is special, for two reasons: 
- because it creates something out of nothing. In a sense, the network has learnt to map pure noise to the closest artifacts of real data, based on the distribution of input data that the model was trained on. In my mind, the random instantiation of pure noise might, by pure luck, contain some artifacts or patterns that are somewhat close to some artifacts seen in input data (a la Jesus Christ on the toast, or the Virgin Mary cloud). The fist few steps of the diffusion inference process are crucial in determining the final output because they form the macro features of the data, while later denoising steps only refine the data sample, i.e. add finer details. 
- were it not for this step, we can envision expressing the rest of the diffusion model literally as a sequence of denoising autoencoders, where each autoencoder is responsible for some amount of denoising. It is this first denoising step that doesn't look like a denoising autoencoder.
